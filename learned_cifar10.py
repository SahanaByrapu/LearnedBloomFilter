# -*- coding: utf-8 -*-
"""Learned_CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11L1q2vzX7niEagYtq-A5n6giIL3VW_9c
"""

#Importing the libraries 
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout
from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Model

# Loading the data 
cifar10 = tf.keras.datasets.cifar10

# Partition into Test and Train Data 
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

x_train[0]

# Reducing pixel values
x_train, x_test = x_train / 255.0, x_test / 255.0

# flattening the label values
y_train, y_test = y_train.flatten(), y_test.flatten()

np.unique(y_train)

x_train[0]

# number of classes
K = len(set(y_train))

# calculate total number of classes
# for output layer
print("number of classes:", K)

# Build the model using the functional API
# input layer
i = Input(shape=x_train[0].shape)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dropout(0.2)(x)

# Hidden layer
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)

# last hidden layer i.e.. output layer
x = Dense(K, activation='softmax')(x)

model = Model(i, x)

# model description
model.summary()

# Model Compile
model.compile(optimizer='adam',
			loss='sparse_categorical_crossentropy',
			metrics=['accuracy'])

# Model Fitting
r = model.fit(
x_train, y_train, validation_data=(x_test, y_test), epochs=50)

# Fit with data augmentation
batch_size = 32
data_generator = tf.keras.preprocessing.image.ImageDataGenerator(
width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)

train_generator = data_generator.flow(x_train, y_train, batch_size)
steps_per_epoch = x_train.shape[0] // batch_size

r = model.fit(train_generator, validation_data=(x_test, y_test),
			steps_per_epoch=steps_per_epoch, epochs=50)

# label mapping

labels = ''''''.split()

# select the image from our test dataset
image_number = 0

# load the image in an array
n = np.array(x_train)

p=[]
# reshape it
for i in range(len(n)):
  p.append(n[i].reshape(1, 32, 32, 3))

len(x_train)

predicted_label=[]
for i in range(50000):
 predicted_label.append(model.predict(p[i]).argmax())

# Commented out IPython magic to ensure Python compatibility.
# %pip install bloom-filter

#Traditional Bloom Filter 
from bloom_filter import BloomFilter

def Train_overflow(bloom,train_features,y_new_train,preds,tau):
    X_train=train_features
    for i in range(len(preds)):
        if preds[i]>tau:
              if y_new_train[i] ==1:
                  bloom.add(str(X_train[i]))
    return bloom

def binarized(Y_train):
  y=[]
  for i in range(len(Y_train)):
        if(Y_train[i]<=4):
          y.append(1)
        else:
          y.append(0)
  return y

tau=4
bloom_traditional = BloomFilter(max_elements=25000)
y_new_train=binarized(y_train)

bloom_overflow=Train_overflow(bloom_traditional,x_train,y_new_train,predicted_label,tau)

# loading the image in an array
n = np.array(x_test)
p=[]
# reshape it
for i in range(len(n)):
  p.append(n[i].reshape(1, 32, 32, 3))

predicted_tst=[]
for i in range(len(x_test)):
  predicted_tst.append(model.predict(p[i]).argmax())

pred_new_test=binarized(predicted_tst)

count=0
for i in range(len(predicted_tst)):
   if predicted_tst[i]!=y_test[i]:
     count+=1
print(count)

def positive_negative(X,Y,tau,predicted_tst):
    keys=[]
    non_keys=[]
    pred_keys=[]
    pred_non_keys=[]
    actual_for_keys=[]
    actual_for_non_keys=[]
    print("positive",len(Y))
    count1=0
    count2=0
    for i in range(len(predicted_tst)):
     if predicted_tst[i]<=tau:
        keys.append(X[i])
        pred_keys.append(1)
        if predicted_tst[i]!=Y[i]:
           actual_for_keys.append(Y[i])
           count1+=1
           #print(count1)
     elif str(X[i]) in bloom_overflow:
           keys.append(X[i])
           actual_for_keys.append([i,Y[i]])
           count2+=1
     else:
          non_keys.append(X[i])
          pred_non_keys.append(0)
          actual_for_non_keys.append([i,Y[i]])
           #print(count2)
    return keys,non_keys,pred_keys,pred_non_keys,actual_for_keys,actual_for_non_keys,count1,count2

def Test_BF_FNR(model,bloom_overflow,non_keys,tau,prediction):
      output1=[]
      FN=0
      for i in range(len(non_keys)):
          if str(non_keys[i]) in bloom_overflow:
              output1.append(1)
              FN+=1
          else:
              output1.append(0)
      
      return np.array(output1),FN

tau=4
keys,non_keys,pred_keys,pred_nonkeys,actual_for_keys,actual_for_non_keys,count1,count2=positive_negative(x_test,y_test,tau,predicted_tst)
keys=np.array(keys)

print("FNR",count1/len(keys),"FPR",count2/len(non_keys))

def Test_BF(bloom1, test_data):
  y_pred_bloom = []
  for i in test_data:
    if str(i) in bloom1:
      y_pred_bloom.append(1)
    else:
      y_pred_bloom.append(0)
  y_pred_bloom = np.array(y_pred_bloom)
  return y_pred_bloom

#creating traditional bloom filter
bloom4 =BloomFilter(max_elements=325000)
dataset=[]
for i in range(len(x_train)):
   dataset.append([x_train[i],y_new_train[i]]) 
for data_point in dataset:
        if data_point[1]==1:
            bloom4.add(data_point[0].all())

Y_pred_bloom_tr=Test_BF(bloom4,x_train)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_new_train,Y_pred_bloom_tr))

Y_pred_bloom=Test_BF(bloom4,x_test)

print(accuracy_score(Y_pred_bloom,y_new_test))

#Calculating true negatives
TN=0
for i in range(len(y_new_test)):
  if y_new_test[i] == 0:
       TN+=1
print("TN",TN)
#Calculating true positives
TP=0
for i in range(len(y_new_test)):
  if y_new_test[i] == 1:
       TP+=1
print("TP",TP)

FN=Test_FNR_bloom(Y_pred_bloom,y_new_test)
if(FN==0):
  print("zero false negative rate")
else:
  print(FN,FN/(FN+TP))
FP=Test_FPR_bloom(Y_pred_bloom,y_new_test)
print(FP,FP/(FP+TN))

def Test_FPR_bloom(prediction,y_new_test):
      output1=[]
      count1=0
      for i in range(len(x_test)):
             if(y_new_test[i]==0 and prediction[i]==1):
               count1+=1
      print(count1)
      return count1

def Test_FNR_bloom(y_pred,y_new_test):
    count1=0
    for i in range(len(y_new_test)):
          if y_new_test[i] == 1 and y_pred[i]==0:
             count1+=1

    if count1==TN:
        return 0
    else:
        return TN-count1