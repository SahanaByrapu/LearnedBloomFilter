# -*- coding: utf-8 -*-
"""Learned_second.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZlZECVnFbchLM326XLvXb-5bKSrWO-F
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import classification_report,accuracy_score
from nltk.tokenize import sent_tokenize, word_tokenize
import gensim
from gensim.models import Word2Vec
import nltk
nltk.download('punkt')
import warnings
warnings.filterwarnings(action = 'ignore')
%pip install bloom_filter
%pip install tensorflow_hub
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from bloom_filter import BloomFilter


Traindata_df = pd.read_csv("/content/malicious_phish.csv")
X=Traindata_df['url']
Y=Traindata_df['type']
Y_new=[]
count1=0
count2=0
for row in Y:
    if row=='benign':
       row=1
       count1+=1
    else:
       row=0
       count2+=1
    Y_new.append(row)

print("total records:",len(Y_new))
print("non-maliciuos urls(labeled as 1):", count1)
print("malicious urls(labeled as 0):",count2)


data=[]
for row in X:
	row=sent_tokenize(row)
	#print(row)
	temp = []
	# tokenize the sentence into words
	for j in word_tokenize(str(row[0])):
		temp.append(j.lower())
		
	#print(temp)
	data.append(temp)

# Create CBOW model
model1 = gensim.models.Word2Vec(data, min_count = 1,
							size = 100, window = 5)

# integer encode the documents
vocab_size = len(model1.wv.vocab)
encoded_docs = [one_hot(d, vocab_size) for d in X]
print(encoded_docs[0])


# pad documents to a max length of 4 words
max_length = len(max(encoded_docs, key=len))
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)

import numpy as np
Y=np.array(Y_new)
X_train, X_test, Y_train, Y_test = train_test_split(padded_docs, Y, test_size=0.3, random_state=42)


def malicious(X,Y):
    keys=[]
    nonkeys=[]
    pred_nonkeys=[]
    print("malicious",len(Y))
    for i in range(len(Y)):
       if(Y[i]== 0):
         keys.append(X[i])
       else:
         nonkeys.append(X[i])
         pred_nonkeys.append(Y[i])
         #print(keys,X[i])
    return keys,nonkeys,pred_nonkeys

def hashfunc(y_pred, m):
    d=np.floor(y_pred*m).astype(int)
    return d

def bitmap(y_pred,m):
    M=[]
    M=[0]*m
    
    d=hashfunc(y_pred,m)
    #print("index",d)
    for i in d:
       #print(i)
       M[i]=1
    return M

from sklearn.linear_model import LinearRegression
model_lr = LinearRegression()

model_lr.fit( X_train , Y_train )
keys,non_keys,dummy=malicious(X_train,Y_train)
#print(len(keys))
prediction_lr = model_lr.predict(keys)

for i in range(len(prediction_lr)):
    if prediction_lr[i] >=1:
      prediction_lr[i]=0.99

m=325000
M=bitmap(prediction_lr, m)
#print(len(M),M)

print("FPR")
prediction_lr_test = model_lr.predict(X_test)
keys,non_keys,pred_nonkeys=malicious(X_test,prediction_lr_test)

print(pred_nonkeys[:30])
print(np.max(pred_nonkeys))
print(len(pred_nonkeys))

count=0
for i in range(len(pred_nonkeys)):
    if pred_nonkeys[i] >=1:
      pred_nonkeys[i]=0.99
      count+=1

print(count)
print(pred_nonkeys[:30])
print(np.max(pred_nonkeys))
print(len(pred_nonkeys))

c=np.floor(pred_nonkeys[1]*m).astype(int)
print(c)

sum=0
c=[]
#d=hashfunc(pred_nonkeys,m)
#sum+=d
#FPR=sum/len(pred_non_keys)
#print(len(pred_nonkeys))
for i in range(len(pred_nonkeys)):
    c.append(np.floor(pred_nonkeys*m).astype(int))
    print(c)
#d=np.floor(pred_nonkeys*m).astype(int)
#sum+=M[d]
print(sum)
#print(len(M),M)
